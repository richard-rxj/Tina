#!/usr/bin/env python# encoding: utf-8from Twitter.twitter_util import TweetCommonUtilfrom Twitter.tweet_tweetapi import TweetApiHelperfrom Twitter.tweet_tweepy import TweepyHelperfrom Twitter.tweet_model import CompanyRecordfrom Twitter.tweet_model import TweetRecordfrom Twitter.tweet_db import SqlLiteHelperimport pandas as pdclass TweetMain:    def analyze(output_file_name, input_file_name, tweet_column_index):        input_user_infos = TweetCommonUtil.read_info_from_csv(input_file_name, tweet_column_index)        existing_user_infos_in_output = TweetCommonUtil.read_info_from_csv(output_file_name, tweet_column_index)        print("-------")        if (len(existing_user_infos_in_output) <= 0):            title = ["handID", "CONAME", "gvkey", "Company_twitter", "Company_type", "Company_twitter_month",                     "Company_twitter_day", "Company_twitter_year", "Company_twitter_date",                     "TWITTER_ACCOUNT_ID", "Number_follower", "Number_following",                     "Tweet_id_str", "Tweet_time", "Tweet_text", "likes", "retweets", "Is a reply", "reply_tweet_id",                     "reply_tweet_text", "Is a retweet", "retweet_tweet_id", "Is a quote", "quote_tweet_id"]            TweetCommonUtil.to_csv(title, output_file_name, False)        total = len(input_user_infos)        index = len(existing_user_infos_in_output)        for screen_name in input_user_infos.keys():            if screen_name in existing_user_infos_in_output:                continue            index = index + 1            print(str(total) + "-" + str(index) + "_" + screen_name)            origin_input_info = input_user_infos[screen_name]            tweet_account_info = TweepyHelper.get_tweet_info(screen_name)            account_only = False            if (len(tweet_account_info) > 1):                # all_tweets=get_alltweets_by_user(api,screen_name)                all_tweets = TweetApiHelper.get_tweets_tweetAPI(screen_name)                if len(all_tweets) < 1:                    account_only = True                else:                    for single_tweet in all_tweets:                        to_output = []                        to_output.extend(origin_input_info)                        to_output.extend(tweet_account_info)                        to_output.extend(single_tweet)                        TweetCommonUtil.to_csv(to_output, output_file_name, True)            else:                account_only = True            if account_only:                to_output = []                to_output.extend(origin_input_info)                to_output.extend(tweet_account_info)                TweetCommonUtil.to_csv(to_output, output_file_name, True)    def generate_powertrack_rulefile(output_file_name, input_file_name, tweet_column_index):        input_user_infos = TweetCommonUtil.read_info_from_csv(input_file_name, tweet_column_index)        TweetApiHelper.generate_powertrack_rule(input_user_infos.keys(), output_file_name)    def sub_process(input_records, output_file_name, company_dict, existed_tweet_ids):        output_records = []        tweet_records = []        for input_record in input_records:            try:                tweet_id = input_record['Tweet_id_str']                company_screen_name=input_record['Company_twitter']                if tweet_id in existed_tweet_ids:                    continue                existed_tweet_ids.add(tweet_id)                company_record=company_dict.get(company_screen_name)                if company_record is None:                    company_record=CompanyRecord.parse_dict(input_record)                    company_dict[company_screen_name]=company_record                tweet_record=TweetRecord.parse_dict(input_record)                company_record.update_by_tweet_record(tweet_record)                tweet_record.company_record=company_record                tweet_records.append(tweet_record)                input_record["total_word"]=tweet_record.total_word                input_record["tone_analysis"]=tweet_record.tone_analysis                input_record["is_earnings_related"]=tweet_record.is_earnings_related                input_record["is_investor_related"]=tweet_record.is_investor_related                output_records.append(input_record)            except ValueError as e:                print(input_record["CONAME"]+"----"+str(tweet_id))        SqlLiteHelper.save_tweet_record(tweet_records)        pd.DataFrame(data=output_records).to_csv(output_file_name, index=False)    def further_process(input_tweet_file, output_file_prefix):        chunksize=50000        chunk_index=0        company_dict={}        existed_tweet_ids=set()        for gm_chunk in pd.read_csv(input_tweet_file, chunksize=chunksize, error_bad_lines=False, warn_bad_lines=True):            chunk_index=chunk_index+1            output_file_name=output_file_prefix+"_"+str(chunk_index)+".csv"            input_data_frame_replace_nan = gm_chunk.replace({pd.np.nan: None})            input_records = input_data_frame_replace_nan.to_dict('records')            TweetMain.sub_process(input_records, output_file_name, company_dict, existed_tweet_ids)        SqlLiteHelper.save_company_record(company_dict.values())        TweetCommonUtil.output_company_records_csv(company_dict.values(), output_file_prefix + "_overall.csv")    def initial_process(input_company_file, output_file_prefix):        company_dict={}        # read existing company from db        for existing_company_record in SqlLiteHelper.get_all_company_records():            company_dict[existing_company_record.company_screen_name]=existing_company_record        companys_to_process=pd.read_csv(input_company_file, error_bad_lines=False, warn_bad_lines=True).replace({pd.np.nan: None}).to_dict('record')        for company_to_process in companys_to_process:            company_record_to_process=CompanyRecord.parse_dict(company_to_process)            if company_record_to_process.company_screen_name in company_dict:                company_record_to_process=company_dict[company_record_to_process.company_screen_name]            else:                company_dict[company_record_to_process.company_screen_name]=company_record_to_process            if company_record_to_process.company_screen_name is not None:                TweepyHelper.get_tweet_info(TweetCommonUtil.get_screen_name(company_record_to_process.company_screen_name), company_record_to_process)        TweetCommonUtil.output_company_records_csv(company_dict.values(), output_file_prefix + "_overall.csv")if __name__ == '__main__':    output_file_name = "201901_output.csv"    input_file_name = "20180226_input.csv"    tweet_column_index = 3    # analyze(output_file_name, input_file_name, tweet_column_index)    #TweetMain.generate_powertrack_rulefile("rule_tina_uq.json",input_file_name,tweet_column_index)    #TweetMain.further_process("20180226_output_20180507.csv", "test_output")    TweetMain.initial_process("20180226_input.csv", "tweet_20180226")